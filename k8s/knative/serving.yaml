# Knative Serving Installation for K3s
#
# Prerequisites:
#   - K3s cluster with Cilium CNI
#   - cert-manager installed
#
# Installation Steps:
#   1. Install Knative Serving CRDs and core:
#      kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.12.0/serving-crds.yaml
#      kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.12.0/serving-core.yaml
#
#   2. Install Kourier (lightweight ingress for Knative):
#      kubectl apply -f https://github.com/knative/net-kourier/releases/download/knative-v1.12.0/kourier.yaml
#
#   3. Configure Knative to use Kourier:
#      kubectl patch configmap/config-network \
#        --namespace knative-serving \
#        --type merge \
#        --patch '{"data":{"ingress-class":"kourier.ingress.networking.knative.dev"}}'
#
#   4. Apply this file:
#      kubectl apply -f k8s/knative/serving.yaml
#
---
# Knative Serving Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-features
  namespace: knative-serving
data:
  # Enable scale-to-zero (serverless)
  scale-to-zero-grace-period: "30s"
  # Enable init containers for ko builds
  kubernetes.podspec-init-containers: "enabled"
  # Enable affinity/anti-affinity
  kubernetes.podspec-affinity: "enabled"
  # Enable tolerations
  kubernetes.podspec-tolerations: "enabled"
  # Enable node selectors
  kubernetes.podspec-nodeselector: "enabled"
  # Enable security context
  kubernetes.podspec-securitycontext: "enabled"
  # Enable persistent volume claims (for stateful services if needed)
  kubernetes.podspec-persistent-volume-claim: "enabled"
  kubernetes.podspec-persistent-volume-write: "enabled"
---
# Autoscaler configuration for scale-to-zero
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-autoscaler
  namespace: knative-serving
data:
  # Enable scale to zero
  enable-scale-to-zero: "true"
  # Time to wait before scaling to zero
  scale-to-zero-grace-period: "30s"
  # Pod retention period after scale down
  scale-to-zero-pod-retention-period: "0s"
  # Stable window for autoscaling decisions
  stable-window: "60s"
  # Panic window for rapid scaling
  panic-window-percentage: "10.0"
  # Panic threshold
  panic-threshold-percentage: "200.0"
  # Target burst capacity
  target-burst-capacity: "200"
  # Requests per second target per container
  container-concurrency-target-percentage: "70"
  # Max scale up rate
  max-scale-up-rate: "1000.0"
  # Max scale down rate
  max-scale-down-rate: "2.0"
---
# Domain configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-domain
  namespace: knative-serving
data:
  # Default domain - replace with your actual domain
  # Using sslip.io for local development (resolves to IP in subdomain)
  sslip.io: ""
  # Example for custom domain:
  # yourdomain.com: |
  #   selector:
  #     app: production
---
# Deployment configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-deployment
  namespace: knative-serving
data:
  # Progress deadline for deployments
  progress-deadline: "600s"
  # Digest resolution - use tagged images directly (ko compatibility)
  registries-skipping-tag-resolving: "kind.local,ko.local,dev.local,ghcr.io"
  # Queue sidecar image (handles traffic routing)
  queue-sidecar-cpu-request: "25m"
  queue-sidecar-cpu-limit: "100m"
  queue-sidecar-memory-request: "50Mi"
  queue-sidecar-memory-limit: "200Mi"
---
# Network configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-network
  namespace: knative-serving
data:
  # Use Kourier as ingress
  ingress-class: "kourier.ingress.networking.knative.dev"
  # Enable auto TLS with cert-manager
  auto-tls: "Enabled"
  # Use HTTP/2 for better performance
  http-protocol: "Redirected"
  # Cluster local domain suffix
  domain-template: "{{.Name}}.{{.Namespace}}.{{.Domain}}"
  # Tag template for traffic splitting
  tag-template: "{{.Tag}}-{{.Name}}"
---
# Defaults configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-defaults
  namespace: knative-serving
data:
  # Default revision timeout
  revision-timeout-seconds: "300"
  # Max revision timeout
  max-revision-timeout-seconds: "600"
  # Default container concurrency (0 = unlimited)
  container-concurrency: "0"
  # Container name (ko uses 'user-container' by default)
  container-name-template: "user-container"
  # Enable init containers
  allow-container-concurrency-zero: "true"
---
# Observability configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-observability
  namespace: knative-serving
data:
  # Enable request logging
  logging.enable-request-log: "true"
  logging.request-log-template: '{"severity":"INFO","httpRequest":{"requestMethod":"{{.Request.Method}}","requestUrl":"{{.Request.URL}}","requestSize":"{{.Request.ContentLength}}","status":"{{.Response.Code}}","responseSize":"{{.Response.Size}}","latency":"{{.Response.Latency}}s","userAgent":"{{.Request.UserAgent}}","remoteIp":"{{.Request.RemoteAddr}}","protocol":"{{.Request.Proto}}"}}'
  # Metrics backend (prometheus)
  metrics.backend-destination: "prometheus"
---
# Kourier configuration - use internal LB (Tailscale access)
apiVersion: v1
kind: Service
metadata:
  name: kourier
  namespace: kourier-system
  labels:
    networking.knative.dev/ingress-provider: kourier
spec:
  type: ClusterIP  # Internal only - access via Tailscale
  selector:
    app: 3scale-kourier-gateway
  ports:
    - name: http2
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: https
      port: 443
      protocol: TCP
      targetPort: 8443
---
# Cilium Network Policy for Knative Serving
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: knative-serving-policy
  namespace: knative-serving
spec:
  description: "Allow Knative Serving components to communicate"
  endpointSelector: {}
  ingress:
    - fromEntities:
        - cluster
        - host
    - fromEndpoints:
        - matchLabels:
            io.kubernetes.pod.namespace: kourier-system
        - matchLabels:
            io.kubernetes.pod.namespace: production
        - matchLabels:
            io.kubernetes.pod.namespace: staging
        - matchLabels:
            io.kubernetes.pod.namespace: development
  egress:
    - toEntities:
        - cluster
        - host
    - toEndpoints:
        - matchLabels:
            io.kubernetes.pod.namespace: kourier-system
        - matchLabels:
            io.kubernetes.pod.namespace: production
        - matchLabels:
            io.kubernetes.pod.namespace: staging
        - matchLabels:
            io.kubernetes.pod.namespace: development
        - matchLabels:
            io.kubernetes.pod.namespace: cert-manager
    # Allow pulling images
    - toFQDNs:
        - matchPattern: "*.gcr.io"
        - matchPattern: "ghcr.io"
        - matchPattern: "*.pkg.dev"
      toPorts:
        - ports:
            - port: "443"
              protocol: TCP
---
# Cilium Network Policy for Kourier
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: kourier-policy
  namespace: kourier-system
spec:
  description: "Allow Kourier ingress gateway traffic"
  endpointSelector: {}
  ingress:
    - fromEntities:
        - cluster
        - host
        - world
  egress:
    - toEntities:
        - cluster
    - toEndpoints:
        - matchLabels:
            io.kubernetes.pod.namespace: knative-serving
        - matchLabels:
            io.kubernetes.pod.namespace: production
        - matchLabels:
            io.kubernetes.pod.namespace: staging
        - matchLabels:
            io.kubernetes.pod.namespace: development
